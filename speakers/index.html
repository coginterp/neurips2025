<!DOCTYPE html>
<html lang="en"><head>

    <meta name="generator" content="Hugo 0.147.6">
    <meta name="date" content="2025-05-27T11:17:16Z">
    
    <meta charset="utf-8">
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="referrer" content="no-referrer">
    
    <meta name="author" content=" TODO   " />
    <meta name="description" content="First Workshop on Interpreting Cognition in Deep Learning Models (ICLR 2023)" />
    <meta name="keywords" content="workshop, AI interpretability, cognitive science" />
    
    <title>CogInterp 2025 | Invited Speakers</title>
    
    <meta property="og:title" content="Invited Speakers" />
    <meta property="og:type" content="website" />
    <meta property="og:description" content="First Workshop on Interpreting Cognition in Deep Learning Models (ICLR 2023)" />
    
    <meta name="twitter:title" content="" />
    
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Lato:ital,wght@0,300;0,400;1,300&display=swap" rel="stylesheet"> 
    
    <link rel="canonical" href="https://coginterp.github.io/neurips2025/speakers/">
    <link rel="stylesheet" href="https://coginterp.github.io/neurips2025/styles.css">
    
    <link rel="apple-touch-icon" sizes="180x180" href="https://coginterp.github.io/neurips2025/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="https://coginterp.github.io/neurips2025/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="https://coginterp.github.io/neurips2025/favicon-16x16.png">
    <link rel="manifest" href="https://coginterp.github.io/neurips2025/site.webmanifest">

</head>
<body><section id="header">

    <div id="logo-container">
        <div id="title-inner-container">
            <a href="https://coginterp.github.io/neurips2025/"><img src="https://coginterp.github.io/neurips2025/logo.png" width="60%" id="logo"></a>
        </div>
    </div>

    <div id="title-container">
        <div id="title-inner-container">
            <div class="supertitle">First Workshop on</div>
            <div class="title"><a href="https://coginterp.github.io/neurips2025/"><b>CogInterp</b>: Interpreting Cognition <br> in Deep Learning Models</a></div>
            <div class="subtitle">Dec. 6 or 7 @ NeurIPS 2025</div>
        </div>
    </div>

    <br>

    <div id="navigation">
        <ul>
        
            <li><strong><a href="https://coginterp.github.io/neurips2025/">About</a></strong></li>
        
            <li><strong><a href="https://coginterp.github.io/neurips2025/call-for-papers">Call for Papers</a></strong></li>
        
            <li><strong><a href="https://coginterp.github.io/neurips2025/schedule">Schedule</a></strong></li>
        
            <li><strong><a href="https://coginterp.github.io/neurips2025/speakers">Speakers</a></strong></li>
        
            <li><strong><a href="https://coginterp.github.io/neurips2025/organizers-reviewers">Organizers</a></strong></li>
        
      </ul>
    </div>

</section><section id="content">
        
    <h1 id="invited-speakers">Invited Speakers</h1>
<div class="list-of-people">
    <div class="person">
        <td><img src="https://coginterp.github.io/neurips2025/speakers/siddharth.png"></td>
        <td><a href="https://homepages.inf.ed.ac.uk/snaraya3/" target="_blank">Siddharth N.</a></td>
    </div>
    <div class="person">
        <td><img src="https://coginterp.github.io/neurips2025/speakers/zeynep.jpg"></td>
        <td><a href="https://www.eml-unitue.de/people/zeynep-akata" target="_blank">Zeynep Akata</a></td>
    </div>
    <div class="person">
        <td><img src="https://coginterp.github.io/neurips2025/speakers/arsha.jpeg"></td>
        <td><a href="https://a-nagrani.github.io/" target="_blank">Arsha Nagrani</a></td>
    </div>
    <div class="person">
        <td><img src="https://coginterp.github.io/neurips2025/speakers/paul.jpg"></td>
        <td><a href="https://www.cs.cmu.edu/~pliang/" target="_blank">Paul Pu Liang</a></td>
    </div>
    <div class="person">
        <td><img src="https://coginterp.github.io/neurips2025/speakers/kristen.jpg"></td>
        <td><a href="https://www.cs.utexas.edu/users/grauman/" target="_blank">Kristen Grauman</a></td>
    </div>
</div>
<script>
  var ul = document.querySelector('div.list-of-people');
  for (var i = ul.children.length; i >= 0; i--) {
      ul.appendChild(ul.children[Math.random() * i | 0]);
  }
</script>
<div id="bio-zeynep">
<h2 id="zeynep-akata">Zeynep Akata</h2>
<p>Zeynep Akata is a professor of Computer Science (W3) within the Cluster of
Excellence Machine Learning at the University of Tübingen.  After completing
her PhD at the INRIA Rhone Alpes with Prof Cordelia Schmid (2014), she
worked as a post-doctoral researcher at the Max Planck Institute for
Informatics with Prof Bernt Schiele (2014-17) and at University of
California Berkeley with Prof Trevor Darrell (2016-17). Before moving to
Tübingen in October 2019, she was an assistant professor at the University
of Amsterdam with Prof Max Welling (2017-19). She received a Lise-Meitner
Award for Excellent Women in Computer Science from Max Planck Society in
2014, a young scientist honour from the Werner-von-Siemens-Ring foundation
in 2019 and an ERC-2019 Starting Grant from the European Commission. Her
research interests include multimodal learning and explainable AI.</p>
</div>
<div id="bio-kristen">
<h2 id="kristen-grauman">Kristen Grauman</h2>
<p>Kristen Grauman is a Professor in the Department of Computer Science at the
University of Texas at Austin and a Research Director in Facebook AI
Research (FAIR).  Her research in computer vision and machine learning
focuses on video, visual recognition, and action for perception or embodied
AI.  Before joining UT-Austin in 2007, she received her Ph.D. at MIT.  She
is an IEEE Fellow, AAAI Fellow, Sloan Fellow, a Microsoft Research New
Faculty Fellow, and a recipient of NSF CAREER and ONR Young Investigator
awards, the PAMI Young Researcher Award in 2013, the 2013 Computers and
Thought Award from the International Joint Conference on Artificial
Intelligence (IJCAI), the Presidential Early Career Award for Scientists and
Engineers (PECASE) in 2013.  She was inducted into the UT Academy of
Distinguished Teachers in 2017.  She and her collaborators have been
recognized with several Best Paper awards in computer vision, including a
2011 Marr Prize and a 2017 Helmholtz Prize (test of time award).  She served
for six years as an Associate Editor-in-Chief for the Transactions on
Pattern Analysis and Machine Intelligence (PAMI) and for ten years as an
Editorial Board member for the International Journal of Computer Vision
(IJCV).  She also served as a Program Chair of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR) 2015 and a Program Chair of
Neural Information Processing Systems (NeurIPS) 2018, and will serve as a
Program Chair of the IEEE International Conference on Computer Vision (ICCV)
2023.</p>
</div>
<div id="bio-paul">
<h2 id="paul-liang">Paul Liang</h2>
<p>Paul Liang is a Ph.D. student in Machine Learning at CMU, advised by
Louis-Philippe Morency and Ruslan Salakhutdinov. His research lies in the
foundations of multimodal machine learning with applications in socially
intelligent AI, understanding human and machine intelligence, natural
language processing, healthcare, and education. He is a recipient of the
Facebook PhD Fellowship, Center for Machine Learning and Health Fellowship,
and the Alan J. Perlis Graduate Student Teaching Award, and his research has
been recognized by 3 best-paper awards at NeurIPS workshops and ICMI. He
regularly organizes courses, workshops, and tutorials on multimodal machine
learning.</p>
</div>
<div id="bio-arsha">
<h2 id="arsha-nagrani">Arsha Nagrani</h2>
<p>Arsha Nagrani is a senior research scientist at Google AI Research, where
she works on machine learning for video understanding. She got her PhD in
the VGG group with Andrew Zisserman at the University of Oxford, supported
by an EPSRC grant and a Google PhD Fellowship Award. Her thesis “Video
Understanding using Multimodal Deep Learning” won the 2021 ELLIS PhD award.
Her research focuses on self-supervised and multi-modal machine learning
techniques for video recognition, including the use of sound and text to
learn better visual representations.</p>
</div>
<div id="bio-sid">
<h2 id="siddharth-narayanaswamy">Siddharth Narayanaswamy</h2>
<p>Siddharth N. is a Reader in Explainable AI in the School of
Informatics at the University of Edinburgh, a part-time Senior Research
Fellow at the Alan Turing Institute, a Visiting Fellow at the Department of
Engineering Science at the University of Oxford, and an ELLIS Scholar. He
was previously a Senior Researcher in Engineering at the University of
Oxford and a Postdoctoral Scholar in Psychology at Stanford. He obtained his
PhD from Purdue University in Electrical and Computer Engineering.
His research interests are broadly cross-disciplinary and motivated by
problems found at the intersection of machine learning, computer vision,
natural-language processing, cognitive science, robotics, and neuroscience.
In particular, he is interested in unsupervised learning of structured
representations from perceptual data, and establishing common ground between
machines and humans through interaction, with implications for building
robust, generalisable, and interpretable AI and ML systems.</p>
</div>


    </section>
<div id="footer">
    Made with <a href="https://gohugo.io/">Hugo</a> and hosted on <a href="https://github.com/coginterp/neurips2025">GitHub</a>.
</div>


</body>

</html>